{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "print(groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's technology landscape, and their importance can be understood from several perspectives:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models enable rapid processing of large amounts of text data. This is particularly important in applications where speed is critical, such as real-time chatbots, voice assistants, and sentiment analysis. By quickly analyzing and generating text, these models can respond to user queries and provide accurate results in a timely manner.\n",
      "\n",
      "2. **Scalability**: As the volume of text data increases, fast language models become even more essential. They allow developers to handle large datasets and perform tasks such as text classification, language translation, and named entity recognition at scale. This scalability is critical for applications that process vast amounts of text data, such as social media monitoring, content moderation, and document analysis.\n",
      "\n",
      "3. **Low Latency**: Fast language models enable low-latency interactions, which is vital for applications that require immediate responses, such as voice assistants, customer service chatbots, and language translation apps. Low latency ensures a seamless user experience, as users can quickly receive responses to their queries without noticeable delays.\n",
      "\n",
      "4. **Resource Efficiency**: Fast language models can run on less powerful hardware, reducing the need for expensive and energy-intensive computing resources. This is particularly important for edge computing, mobile devices, and other resource-constrained environments where computational power is limited. By optimizing language models for speed, developers can deploy models on a wider range of devices, expanding their reach and accessibility.\n",
      "\n",
      "5. **Real-time Applications**: Fast language models are essential for real-time applications, such as:\n",
      "\t* Live subtitles and closed captions for videos and audio content.\n",
      "\t* Real-time sentiment analysis for social media monitoring and brand reputation management.\n",
      "\t* Instant language translation for conversations and meetings.\n",
      "\t* Live chat and customer support bots that require rapid response times.\n",
      "\n",
      "6. **Competitive Advantage**: In many industries, fast language models can provide a competitive advantage. For example, a company that can quickly analyze customer feedback and respond to concerns in real-time is more likely to build trust and loyalty with its customers. Similarly, a business that can rapidly translate and localize content can expand its reach and tap into new markets more efficiently.\n",
      "\n",
      "7. **Research and Development**: Fast language models can accelerate research and development in various fields, such as:\n",
      "\t* Natural Language Processing (NLP): Fast models enable researchers to quickly experiment with new ideas, iterate on existing models, and explore new applications.\n",
      "\t* Human-Computer Interaction (HCI): Fast language models can facilitate more natural and intuitive human-computer interactions, enabling researchers to study and improve these interactions.\n",
      "\n",
      "In summary, fast language models are crucial for efficient processing, scalability, low latency, resource efficiency, real-time applications, competitive advantage, and research and development. As the demand for language models continues to grow, the importance of fast and efficient models will only continue to increase.\n"
     ]
    }
   ],
   "source": [
    "client = Groq(\n",
    "    api_key=groq_api_key,\n",
    ")\n",
    "\n",
    "#note que o groq pega uma lista de mensagens, não uma única string\n",
    "chat_completion = client.chat.completions.create(\n",
    "    #primeiro parametro é a lista de mensagens\n",
    "    messages=[\n",
    "\n",
    "        {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "\n",
    "        }\n",
    "\n",
    "    ],\n",
    "    #segundo parametro é o modelo de linguagem que será usado (é possível setar outro modelo que a API hospeda)\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora vamos criar uma classe de Agente\n",
    "class Agent:\n",
    "    def __init__(self, client, system):\n",
    "        self.client = client #o cliente do groq\n",
    "        self.system = system #o system prompt\n",
    "        self.messages = [] #lista de mensagens\n",
    "        if self.messages is not None:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": self.system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=self.messages,\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "        )\n",
    "        return completion.choices[0].message.content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
