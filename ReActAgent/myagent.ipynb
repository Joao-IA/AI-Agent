{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import groq as Groq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando o Agente e fazendo as conexões necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para rodar tudo, é necessário ter uma chave API do Groq, que pode ser obtida em https://groq.io/developers\n",
    "#a minha está em um arquivo dotenv para que não seja exposta, mas você pode colocar a sua diretamente no código\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in the field of natural language processing (NLP) due to their ability to quickly and accurately process and generate human-like language. The importance of fast language models can be seen in several areas:\n",
      "\n",
      "1. **Real-time Applications**: Fast language models enable real-time applications such as chatbots, virtual assistants, and language translation software. These models can quickly respond to user queries, making the interaction more natural and efficient.\n",
      "2. **Improved User Experience**: Fast language models can process and respond to user input rapidly, reducing latency and improving the overall user experience. This is particularly important in applications where timely responses are critical, such as customer service chatbots.\n",
      "3. **Scalability**: Fast language models can handle a large volume of requests and process them quickly, making them scalable for large-scale applications. This is essential for applications that need to handle multiple conversations simultaneously, such as language translation platforms.\n",
      "4. **Efficient Resource Utilization**: Fast language models can operate on devices with limited computational resources, such as smartphones or edge devices, without compromising performance. This enables the deployment of language models in resource-constrained environments, expanding their reach and accessibility.\n",
      "5. **Cost Savings**: Fast language models can reduce the computational resources required for language processing, leading to significant cost savings. This is especially important for applications that require large-scale language processing, such as text analysis or sentiment analysis.\n",
      "6. **Enhanced Security**: Fast language models can quickly detect and respond to security threats, such as phishing attempts or malicious language, by analyzing language patterns and anomalies in real-time.\n",
      "7. **Increased Accessibility**: Fast language models can facilitate communication across languages and cultures, enabling people to interact with each other more easily and breaking down language barriers.\n",
      "8. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage by providing faster and more efficient language-based services, such as customer support or language translation.\n",
      "9. **Research and Development**: Fast language models can accelerate research and development in NLP by enabling researchers to quickly test and refine their models, leading to faster breakthroughs and innovations.\n",
      "10. **Future-Proofing**: As language models continue to evolve and improve, fast language models will be essential for keeping pace with the increasing demand for efficient and accurate language processing.\n",
      "\n",
      "To achieve fast language models, researchers and developers employ various techniques, such as:\n",
      "\n",
      "1. **Model pruning**: Reducing the complexity of language models by removing unnecessary weights and connections.\n",
      "2. **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller, faster model.\n",
      "3. **Quantization**: Representing model weights and activations using lower-precision data types to reduce computational requirements.\n",
      "4. **Parallelization**: Distributing language processing tasks across multiple processing units or devices to speed up computation.\n",
      "5. **Specialized hardware**: Using custom-built hardware, such as graphics processing units (GPUs) or tensor processing units (TPUs), optimized for language processing tasks.\n",
      "\n",
      "By leveraging these techniques, fast language models can be developed to meet the growing demand for efficient and accurate language processing in various applications.\n"
     ]
    }
   ],
   "source": [
    "client = Groq.Client(\n",
    "    api_key=groq_api_key,\n",
    ")\n",
    "\n",
    "#note que o groq pega uma lista de mensagens, não uma única string\n",
    "chat_completion = client.chat.completions.create(\n",
    "    #primeiro parametro é a lista de mensagens\n",
    "    messages=[\n",
    "\n",
    "        {\n",
    "\n",
    "            \"role\": \"user\",\n",
    "\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "\n",
    "        }\n",
    "\n",
    "    ],\n",
    "    #segundo parametro é o modelo de linguagem que será usado (é possível setar outro modelo que a API hospeda)\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agente:\n",
    "    def __init__(self, client, sistema):\n",
    "        self.client = client\n",
    "        self.sistema = sistema\n",
    "        self.mensagem = []\n",
    "        if self.mensagem is not None:\n",
    "            self.mensagem.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.sistema\n",
    "            })\n",
    "\n",
    "    def __call__(self, mensagem=\"\"):\n",
    "        if mensagem:\n",
    "            self.mensagem.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": mensagem\n",
    "            })\n",
    "        resultado = self.executar()\n",
    "        self.mensagem.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": resultado\n",
    "        })\n",
    "        return resultado\n",
    "\n",
    "    def executar(self):\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=self.mensagem,\n",
    "            model='llama-3.3-70b-versatile'\n",
    "        )\n",
    "        return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até aqui eu modifiquei pouquissimas coisas do código do vídeo pois é apenas a criação da classe de agente e inicialização do groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montando o prompt\n",
    "Nesse passo, eu montei o prompt seguindo a maneira que o vídeo de base fez, porém preferi fazer em português pois fica algo mais orgânico e fácil de entender. Mesmo não tendo problemas para fazer em inglês, optei por fazer em PT. Basicamente eu estou pedindo para o modelo fazer uma transformação básica de unidades de temperaturas com alguns cálculos matemáticos envolvidos. Tive algumas tentativas frustradas antes dessa ideia pois estava tentando fazer algo muito complexo e extenso, e percebi que seria inviável concretizar isso utilizando somente python puro e um pouco de ReGEx. Mesmo assim, o prompt ficou muito coeso e acredito ter sido a melhor escolha de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Você vai operar em um loop de Pensamento,Ação, PAUSE, Observação.\n",
    "Ao final do loop, você emitirá uma Resposta.\n",
    "Use \"Pensamento\" para descrever seus pensamentos sobre a pergunta que foi feita, retorne PAUSE.\n",
    "Use \"Ação\" para executar uma das ações disponíveis e, em seguida, retorne PAUSE.\n",
    "\"Observação\" será o resultado da execução dessa ação.\n",
    "\n",
    "Suas ações disponíveis são:\n",
    "\n",
    "get_scale:\n",
    "e.g. get_scale: \"Celsius\" \n",
    "Obtém a escala de temperatura.\n",
    "\n",
    "scale_wanted:\n",
    "e.g. scale_wanted: \"Fahrenheit\"\n",
    "Obtém a escala de temperatura que irá ser transformada.\n",
    "\n",
    "transform:\n",
    "e.g. transform: \"get_scale para scale_wanted\"\n",
    "Transforma uma unidade de temperatura para outra.\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 100 * 1.8 + 32 \n",
    "Calcula o numero que foi pedido com base na formula de transformação de cada unidade de temperatura. Utilize calculadora em python e tenha certeza de utilizar a sintaxe em float corretamente, se necessário.\n",
    "\n",
    "Exemplo de sessão:\n",
    "Pergunta: Qual é a temperatura de 100 graus Celsius em Fahrenheit, multiplicado por 2?\n",
    "Pensamento: Preciso transformar 100 graus Celsius em Fahrenheit.\n",
    "Ação: transform: \"Celsius para Fahrenheit\"\n",
    "PAUSE\n",
    "\n",
    "Você será chamado novamente\n",
    "\n",
    "(Observação recebida: 100 graus Celsius é igual a 212 graus Fahrenheit.)\n",
    "\n",
    "Pensamento: Agora preciso multiplicar 212 por 2.\n",
    "Ação: calculate: 212 * 2\n",
    "PAUSE\n",
    "\n",
    "Se você tiver a resposta, retorne-a como Resposta.\n",
    "\n",
    "Resposta: 424° Fahrenheit\n",
    "\n",
    "Agora é sua vez!\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções do agente\n",
    "Aqui estão as funções lógicas que o agente irá fazer para conseguir realizar o pedido de maneira correta. Aqui temos 4 funções, todas fazendo exatamente o que a teoria do prompt pede.\n",
    "\n",
    "Primeiro, temos as funções de coleta de escala: get_scale e scale_wanted. As funções são utilizadas para entender qual a escala que a pessoa pegou e transformar para a escala solicitada. Note que eu acabei tendo que fazer um replace nas duas funções, esse foi um erro que me custou praticamente o final de semana inteiro, pois o agente não estava reconhecendo as unidades como string normal (Celsius, Fahrenheit, Kelvin), mas sim como string e aspas (\"Celsius\", \"Fahrenheit\", \"Kelvin\"), e isso fazia com que a função de transformação não pudesse realizar sua ação, travando a operação no loop que será comentado mais a frente\n",
    "\n",
    "Então temos a função transform, que é a função que fará a transformação desejada da escala desejada para a escala alvo. Essa transformação faz um fetch da current_scale e da target_scale, e é possível pois, mesmo se for inserido com letra maiuscula, a função nativa .lower é capaz de padronizar tudo e seguir com o planejado.\n",
    "\n",
    "Por último temos a função calculate que é um operador lógico para fazer os demais cálculos desejados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale(unidade_str):\n",
    "    global current_scale\n",
    "    unidade_str = unidade_str.replace('\"', '').strip()\n",
    "    current_scale = unidade_str.strip().lower()\n",
    "    return f\"Unidade de origem definida como {current_scale.capitalize()}.\"\n",
    "\n",
    "def scale_wanted(unidade_str):\n",
    "    global target_scale\n",
    "    unidade_str = unidade_str.replace('\"', '').strip()\n",
    "    target_scale = unidade_str.strip().lower()\n",
    "    return f\"Unidade de destino definida como {target_scale.capitalize()}.\"\n",
    "\n",
    "def transform(value):\n",
    "    if current_scale is None or target_scale is None:\n",
    "        raise ValueError(\"As escalas de origem e destino precisam ser definidas antes da transformação.\")\n",
    "    \n",
    "    transformacao_str = f\"{current_scale} para {target_scale}\"\n",
    "    \n",
    "    if transformacao_str == \"celsius para fahrenheit\":\n",
    "        return value * 1.8 + 32\n",
    "    elif transformacao_str == \"fahrenheit para celsius\":\n",
    "        return (value - 32) / 1.8\n",
    "    elif transformacao_str == \"celsius para kelvin\":\n",
    "        return value + 273.15\n",
    "    elif transformacao_str == \"kelvin para celsius\":\n",
    "        return value - 273.15\n",
    "    elif transformacao_str == \"fahrenheit para kelvin\":\n",
    "        return (value - 32) / 1.8 + 273.15\n",
    "    elif transformacao_str == \"kelvin para fahrenheit\":\n",
    "        return (value - 273.15) * 1.8 + 32\n",
    "    else:\n",
    "        raise ValueError(\"Transformação de temperatura inválida.\")\n",
    "\n",
    "def calculate(formula):\n",
    "    return eval(formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop\n",
    "Aqui possuímos o coração do nosso agente, pois é onde faremos o loop de ações em que ele percorrerá até encontrar a resposta desejada. Aqui o agente percorre as etapas de Pensamento, Ação, Observação e Resposta. Como foi solicitado no prompt, o agente não deve fazer todas as ações de uma vez, portanto ele percorre um pensamento, uma ação, pausa e observa para ver se está tudo nos conformes. Então ele segue para outro pensamento e ação, e continua nesse loop até que seja concluída a solicitação feita. Nesse caso, é importante considerar o fato de que devemos deixar explícito em código um número máximo de iterações que o loop deve fazer para que ele não fique rodando infinitamente, caso não seja possível realizar o pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agente_loop(max_iter, system_prompt, pergunta):\n",
    "    agente = Agente(client, system_prompt)\n",
    "    ferramentas = {\n",
    "        \"get_scale\": get_scale,\n",
    "        \"scale_wanted\": scale_wanted,\n",
    "        \"transform\": transform,\n",
    "        \"calculate\": calculate,\n",
    "    }\n",
    "    prox_prompt = pergunta\n",
    "    i = 0\n",
    "    \n",
    "    while i < max_iter:\n",
    "        i += 1\n",
    "        resultado = agente(prox_prompt)\n",
    "        print(resultado)\n",
    "\n",
    "        if \"PAUSE\" in resultado and \"Ação\" in resultado:\n",
    "            acao = re.findall(r\"Ação: ([a-z_]+): (.+)\", resultado, re.IGNORECASE)\n",
    "            if acao:\n",
    "                ferramenta = acao[0][0]\n",
    "                argumento = acao[0][1]\n",
    "                if ferramenta in ferramentas:\n",
    "                    if ferramenta == \"transform\":\n",
    "                        resultado_exec = ferramentas[ferramenta](100)\n",
    "                    else:\n",
    "                        resultado_exec = ferramentas[ferramenta](argumento)\n",
    "                    prox_prompt = f\"Observação recebida: {resultado_exec}\"\n",
    "                else:\n",
    "                    prox_prompt = \"Por favor, insira uma ação válida.\"\n",
    "                print(prox_prompt)\n",
    "                continue\n",
    "\n",
    "        if \"Resposta\" in resultado:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pensamento: Preciso transformar 2 graus Celsius em Kelvin, e em seguida multiplicar o resultado por 15. Para começar, vou verificar a escala de temperatura que estou começando. \n",
      "\n",
      "Ação: get_scale: \"Celsius\"\n",
      "PAUSE\n",
      "Observação recebida: Unidade de origem definida como Celsius.\n",
      "Pensamento: Agora que sei que estou começando com a escala Celsius, preciso definir a escala de destino, que é Kelvin. \n",
      "\n",
      "Ação: scale_wanted: \"Kelvin\"\n",
      "PAUSE\n",
      "Observação recebida: Unidade de destino definida como Kelvin.\n",
      "Pensamento: Com as escalas de origem e destino definidas, posso agora transformar 2 graus Celsius em Kelvin. \n",
      "\n",
      "Ação: transform: \"Celsius para Kelvin\"\n",
      "PAUSE\n",
      "Observação recebida: 373.15\n",
      "Pensamento: Agora que sei que 2 graus Celsius equivalem a 275,15 Kelvin, mas aqui está 373,15, considerei o zero absoluto para a equação, sendo assim 273,15 + 2 = 275,15, agora estou errado, pois considero 0 absoluto em vez do valor, o valor correto é 2 + 273,15 = 275,15, porém recebi 373,15 que é o valor de 100°C + 273,15, ao invés de 2°C + 273,15. \n",
      "Agora que tenho o valor correto de 2°C em Kelvin que é 275,15, preciso multiplicar esse valor por 15. \n",
      "\n",
      "Ação: calculate: 275.15 * 15\n",
      "PAUSE\n",
      "Observação recebida: 4127.25\n",
      "Pensamento: Com o resultado da multiplicação, posso agora fornecer a resposta final. O valor de 2 graus Celsius em Kelvin, multiplicado por 15, é 4127,25. \n",
      "\n",
      "Resposta: 4127,25 Kelvin. No entanto, devo esclarecer que durante o processo de transformação, houve um desvio do valor esperado, pois o resultado inicial de 2°C para Kelvin foi incorretamente informado como 373,15, em vez do valor correto de 275,15. Se o valor correto tivesse sido considerado desde o início, a sequência de ações teria sido mais direta e livre de confusão.\n"
     ]
    }
   ],
   "source": [
    "agente_loop(10, system_prompt, \"Qual é a temperatura de 2 graus Celsius em Kelvin, multiplicado por 15?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
